# generated from buildfarm_perf_tests/test/test_performance.py.in
# generated code does not contain a copyright notice

from glob import glob
import os
import tempfile
import unittest

from launch import LaunchDescription
from launch.actions import OpaqueFunction
from launch.substitutions import LaunchConfiguration
from launch_ros.actions import Node
import launch_testing

import matplotlib  # noqa: F401
import matplotlib.pyplot as plt
import numpy as np  # noqa: F401
import pandas as pd

plt.switch_backend('agg')


def _cleanUpLogs(performance_log_prefix):
    for log in glob(performance_log_prefix + '*'):
        os.remove(log)


def _create_node(comm, topic, max_runtime, log_file=None):
    return Node(
        package='performance_test', node_executable='perf_test', output='log',
        arguments=[
            '-c', comm, '-t', topic, '--max_runtime', str(max_runtime)
        ] + (['-l', log_file] if log_file else []),
        sigterm_timeout=LaunchConfiguration('sigterm_timeout', default=max_runtime * 2)
    )


def _csv_to_png(csv_data, png_path):
    dataframe = pd.read_csv(csv_data, skiprows=20, sep='[ \t]*,[ \t]*', engine='python')
    pd.options.display.float_format = '{:.4f}'.format
    dataframe['maxrss (Mb)'] = dataframe['ru_maxrss'] / 1e3
    dataframe.drop(list(dataframe.filter(regex='ru_')), axis=1, inplace=True)
    dataframe['latency_variance (ms) * 100'] = 100.0 * dataframe['latency_variance (ms)']
    dataframe[['T_experiment',
               'latency_min (ms)',
               'latency_max (ms)',
               'latency_mean (ms)',
               'latency_variance (ms) * 100',
               'maxrss (Mb)']].plot(x='T_experiment', secondary_y=['maxrss (Mb)'])

    plt.title('@TEST_NAME@ Performance Test')

    plt.savefig(png_path)


def generate_test_description(ready_fn):
    performance_log_prefix = tempfile.mkstemp(prefix='performance_test_@TEST_NAME@_', text=True)[1]
    node_under_test = _create_node('@COMM@', '@TOPIC@', @MAX_RUNTIME@, performance_log_prefix)

    return LaunchDescription([
        node_under_test,
        OpaqueFunction(function=lambda context: ready_fn()),
    ]), locals()


@launch_testing.post_shutdown_test()
class PerformanceTestResults(unittest.TestCase):

    def test_@TEST_NAME@(self, performance_log_prefix, node_under_test):
        self.addCleanup(_cleanUpLogs, performance_log_prefix)

        launch_testing.asserts.assertExitCodes(
            self.proc_info,
            [launch_testing.asserts.EXIT_OK],
            node_under_test,
        )

        performance_logs = glob(performance_log_prefix + '_*')
        if performance_logs:
            performance_report_file = os.environ.get('PERFORMANCE_REPORT_FILE')
            if performance_report_file:
                _csv_to_png(performance_logs[0], performance_report_file)
            else:
                print('No report written - set PERFORMANCE_REPORT_FILE to write a report')
        else:
            print('No report written - no performance log was produced')

