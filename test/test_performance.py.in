# generated from buildfarm_perf_tests/test/test_performance.py.in
# generated code does not contain a copyright notice

from glob import glob
import os
import tempfile
import unittest
import xml.etree.cElementTree as ET

from launch import LaunchDescription
from launch.actions import OpaqueFunction
from launch.substitutions import LaunchConfiguration
from launch_ros.actions import Node
import launch_testing

import matplotlib  # noqa: F401
import matplotlib.pyplot as plt
import numpy as np  # noqa: F401
import pandas as pd

plt.switch_backend('agg')


def _cleanUpLogs(performance_log_prefix):
    for log in glob(performance_log_prefix + '*'):
        os.remove(log)


def _create_node(comm, topic, max_runtime, log_file=None):
    return Node(
        package='performance_test', node_executable='perf_test', output='log',
        arguments=[
            '-c', comm, '-t', topic, '--max_runtime', str(max_runtime)
        ] + (['-l', log_file] if log_file else []),
        sigterm_timeout=LaunchConfiguration('sigterm_timeout', default=max_runtime * 2)
    )


def _csv_to_png(csv_data, png_path):
    dataframe = pd.read_csv(csv_data, skiprows=20, sep='[ \t]*,[ \t]*', engine='python')
    pd.options.display.float_format = '{:.4f}'.format
    dataframe['maxrss (Mb)'] = dataframe['ru_maxrss'] / 1e3
    dataframe.drop(list(dataframe.filter(regex='ru_')), axis=1, inplace=True)
    dataframe['latency_variance (ms) * 100'] = 100.0 * dataframe['latency_variance (ms)']
    dataframe[['T_experiment',
               'latency_min (ms)',
               'latency_max (ms)',
               'latency_mean (ms)',
               'latency_variance (ms) * 100',
               'maxrss (Mb)']].plot(x='T_experiment', secondary_y=['maxrss (Mb)'])

    plt.title('@TEST_NAME@ Performance Test')

    plt.savefig(png_path)


def _csv_to_xml(csv_data, xml_path):
    dataframe = pd.read_csv(csv_data, skiprows=20, sep='[ \t]*,[ \t]*', engine='python')
    dataframe_agg = dataframe.agg(['mean', 'std'])

    report = ET.Element('report')
    report.set('name', '@TEST_NAME@ Performance Test')
    report.set('categ', 'Performance Tests')

    test = ET.SubElement(report, 'test')
    test.set('name', '@TEST_NAME@ Performance Test')
    test.set('executd', 'yes')

    result = ET.SubElement(test, 'result')

    #success = ET.SubElement(result, 'success')
    #success.set('passed', 'yes')
    #success.set('state', '100')
    #success.set('hasTimedOut', 'false')

    metrics = ET.SubElement(result, 'metrics')

    latency_mean = ET.SubElement(metrics, 'latency_mean')
    latency_mean.set('unit', 'ms')
    latency_mean.set('measure', str(dataframe_agg.loc['mean', 'latency_mean (ms)']))
    latency_mean.set('isRelevant', 'true')

    latency_variance = ET.SubElement(metrics, 'latency_variance')
    latency_variance.set('unit', 'ms')
    latency_variance.set('measure', str(dataframe_agg.loc['mean', 'latency_variance (ms)']))
    latency_variance.set('isRelevant', 'true')

    tree = ET.ElementTree(report)
    tree.write(xml_path)

def generate_test_description(ready_fn):
    performance_log_prefix = tempfile.mkstemp(prefix='performance_test_@TEST_NAME@_', text=True)[1]
    node_under_test = _create_node('@COMM@', '@TOPIC@', @MAX_RUNTIME@, performance_log_prefix)

    return LaunchDescription([
        node_under_test,
        OpaqueFunction(function=lambda context: ready_fn()),
    ]), locals()


@launch_testing.post_shutdown_test()
class PerformanceTestResults(unittest.TestCase):

    def test_@TEST_NAME@(self, performance_log_prefix, node_under_test):
        self.addCleanup(_cleanUpLogs, performance_log_prefix)

        launch_testing.asserts.assertExitCodes(
            self.proc_info,
            [launch_testing.asserts.EXIT_OK],
            node_under_test,
        )

        performance_logs = glob(performance_log_prefix + '_*')
        if performance_logs:
            performance_report_png = os.environ.get('PERFORMANCE_REPORT_PNG')
            if performance_report_png:
                _csv_to_png(performance_logs[0], performance_report_png)
            else:
                print('No PNG report written - set PERFORMANCE_REPORT_PNG to write a report')
            performance_report_xml = os.environ.get('PERFORMANCE_REPORT_XML')
            if performance_report_xml:
                _csv_to_xml(performance_logs[0], performance_report_xml)
            else:
                print('No XML report written - set PERFORMANCE_REPORT_XML to write a report')
        else:
            print('No report written - no performance log was produced')

